"""
Podcast summarization using OpenAI
"""
import time
import os
from datetime import datetime
from langfuse.openai import openai  # Langfuse OpenAI wrapper for automatic tracing
from langfuse import Langfuse, observe

class PodcastSummarizer:
    def __init__(self):
        self.debug = True
        # Langfuse OpenAI wrapper is automatically configured via env vars
        # No manual OpenAI client initialization needed

        # Initialize Langfuse client for prompt management
        self.langfuse_enabled = os.getenv('LANGFUSE_ENABLED', 'true').lower() == 'true'
        if self.langfuse_enabled:
            try:
                self.langfuse = Langfuse()
            except Exception as e:
                self._debug_log(f"‚ö†Ô∏è  Warning: Failed to initialize Langfuse: {str(e)}")
                self.langfuse_enabled = False
                self.langfuse = None
        else:
            self.langfuse = None
        
    def _debug_log(self, message):
        """Debug logging with timestamp"""
        if self.debug:
            timestamp = datetime.now().strftime("%H:%M:%S")
            print(f"[{timestamp}] SUMMARIZER: {message}")
    
    @observe(as_type="generation", name="podcast_summarization")
    def summarize(self, transcript, title="Podcast Episode"):
        """Generate summary from transcript using OpenAI API with Langfuse Prompt Management"""
        return self._internal_summarize(transcript, title)
    
    def _internal_summarize(self, transcript, title="Podcast Episode"):
        """Core summarization logic shared by both methods"""
        self._debug_log(f"Starting summarization for: {title}")
        self._debug_log(f"Transcript length: {len(transcript)} characters, {len(transcript.split())} words")
        
        # Calculate estimated tokens (rough approximation: 1 token ‚âà 4 characters)
        estimated_input_tokens = len(transcript) // 4
        self._debug_log(f"Estimated input tokens: ~{estimated_input_tokens}")
        
        # Get prompts from Langfuse Prompt Management
        system_prompt_text = ""
        user_prompt_text = ""
        langfuse_prompt = None  # For linking prompt to observation

        try:
            if self.langfuse_enabled and self.langfuse:
                self._debug_log("üìã Fetching prompts from Langfuse Prompt Management...")

                # Get system prompt
                system_prompt_obj = self.langfuse.get_prompt("podcast-analyzer-system", label="production")
                system_prompt_text = system_prompt_obj.prompt

                # Get and compile user prompt with variables
                user_prompt_obj = self.langfuse.get_prompt("podcast-summarization-user", label="production")
                user_prompt_text = user_prompt_obj.compile(
                    title=title,
                    transcript=transcript
                )

                # Store prompt object for automatic linking
                langfuse_prompt = user_prompt_obj

                self._debug_log("‚úÖ Successfully loaded prompts from Langfuse")
            else:
                raise Exception("Langfuse not enabled, using fallback prompts")
                
        except Exception as e:
            self._debug_log(f"‚ö†Ô∏è  Failed to load Langfuse prompts, using fallback: {str(e)}")
            # Fallback to hardcoded prompts
            system_prompt_text = """You are a professional podcast analyst. 
Your job is to create structured summaries of cleaned podcast transcripts. 
Your summaries must adapt to the type of content:

1. If the episode is about PRODUCT MANAGEMENT, AI, TOOLS, or OPERATIONS:
   - Focus on frameworks, methodologies, tools/tech stacks, and clear product/AI insights.
   - Summarize in a way that helps a product manager learn and stay up to date.
   - Provide external links if frameworks/methods are only partially explained.
   - Strip out anecdotes and irrelevant chatter.
   - Leadership lessons only if they are the explicit theme.

2. If the episode is about STRATEGY, STARTUPS, VC, or MARKET TRENDS:
   - Focus on industry insights, investment theses, emerging trends, and market dynamics.
   - Capture the essence of strategic discussions and ecosystem shifts.
   - Provide context on why the trend matters and where to explore further (reports, articles, companies).
   - Do not attempt product/AI tooling detail unless explicitly discussed.

Across all types:
- Provide two layers of summary: quick TL;DR and detailed notes.
- Avoid prescriptive "to-dos" for the user.
- Quotes should be concise and memorable.
- Companies/people should always be noted with context."""
            
            user_prompt_text = f"""
Please analyze this podcast transcript and produce a structured summary with two layers:

---

**TL;DR (quick read):**

‚Ä¢ 3‚Äì5 bullets summarizing the main takeaways (~100 words max)

---

**Detailed Notes:**

**Title:** {title}

**Overview:**
2‚Äì3 sentences on the core theme and relevance.

**Key Topics:**

‚Ä¢ [Main topic 1]
‚Ä¢ [Main topic 2]

IF PRODUCT/AI/OPERATIONAL:
  **Frameworks & Methodologies:**
  
  ‚Ä¢ [Framework/method: explanation or link if vague]

  **Tools & Tech Stacks:**
  
  ‚Ä¢ [Tool/tech: context of use]

  **Key Insights:**
  
  ‚Ä¢ [Insight phrased for product/AI relevance]

IF STRATEGY/VC/TRENDS:
  **Market & Strategic Insights:**
  
  ‚Ä¢ [Trend/insight: why it matters]

  **Investment/Startup Signals:**
  
  ‚Ä¢ [Company/sector: context + thesis]

  **Ecosystem & Future Outlook:**
  
  ‚Ä¢ [Broader implications or where to learn more]

**Actionable Quotes:**

‚Ä¢ "[Memorable quote]"

**Companies/People Mentioned:**

‚Ä¢ [Name: context]

**Cross-Episode Themes (if any):**

‚Ä¢ [Note if connects to previous episode insight]

---

CRITICAL FORMATTING REQUIREMENTS:
1. ALWAYS put a blank line after section headers before bullet points
2. NEVER put bullet points on the same line as headers
3. Each bullet point must be on its own separate line
4. Use this exact format for ALL sections:

CORRECT FORMAT:
**Section Name:**

‚Ä¢ First bullet point here
‚Ä¢ Second bullet point here

WRONG FORMAT:
**Section Name:** ‚Ä¢ First bullet point here
‚Ä¢ Second bullet point here

WRONG FORMAT:
**Section Name:**
‚Ä¢ First bullet point here ‚Ä¢ Second bullet point here

Here is the transcript:
{transcript}
Here is the transcript:
{transcript}
        """
        
        try:
            self._debug_log("üöÄ Sending request to OpenAI API...")
            self._debug_log("üì° Model: gpt-4o-mini (fast & free)")
            self._debug_log("‚öôÔ∏è  Temperature: 0.7 (balanced creativity)")
            self._debug_log("üìù Max output tokens: 2000")
            
            start_time = time.time()
            
            # Prepare messages using the prompt management system
            messages = [
                {"role": "system", "content": system_prompt_text},
                {"role": "user", "content": user_prompt_text}
            ]
            
            # Use Langfuse OpenAI wrapper (automatic tracing + prompt linking)
            response = openai.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                temperature=0.7,
                max_tokens=2000,
                langfuse_prompt=langfuse_prompt  # Links prompt to observation
            )

            # Log prompt source for debugging
            prompt_source = "langfuse_prompt_management" if langfuse_prompt else "fallback"
            self._debug_log(f"üîó Used prompts from: {prompt_source}")
            
            elapsed = time.time() - start_time
            
            # Extract usage information
            usage = response.usage
            input_tokens = usage.prompt_tokens
            output_tokens = usage.completion_tokens
            total_tokens = usage.total_tokens
            
            self._debug_log(f"‚úÖ OpenAI summarization completed in {elapsed:.2f}s")
            self._debug_log(f"üìä Token usage: {input_tokens} input + {output_tokens} output = {total_tokens} total")
            self._debug_log(f"‚ö° Processing speed: {total_tokens/elapsed:.1f} tokens/second")
            
            summary = response.choices[0].message.content
            self._debug_log(f"üìù Summary generated: {len(summary)} characters")
            self._debug_log(f"üìâ Summary ratio: {len(summary)/len(transcript)*100:.1f}% of transcript length")

            return summary
            
        except Exception as e:
            self._debug_log(f"‚ùå Error with OpenAI summarization: {str(e)}")
            raise RuntimeError(f"Failed to generate summary with OpenAI: {str(e)}")
    
